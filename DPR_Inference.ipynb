{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b8722b9-6f3f-40d0-9e1f-fa9324133902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from transformers import *\n",
    "from tqdm import tqdm \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from tqdm.auto import tqdm \n",
    "import faiss \n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7d800a-0ba1-4ac8-b9d0-d0fd01115b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph6vqwl2g\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574a55152bb64d1296678c433db25dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n",
      "creating metadata file for /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n",
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_w1__6uh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba409fb64f664c0781d1779905aa2b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/236k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "creating metadata file for /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpibr80kns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75db9cf92f3747b89af6f954df0ec567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "creating metadata file for /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmckcq2xw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2758f1ac1a8a4143b4085f035e57478f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "creating metadata file for /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a28e84a167423ab1791ac8d6abf3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "\n",
    "questions, answers = [], [] \n",
    "\n",
    "for _, row in tqdm(train.iterrows()):\n",
    "    for q_col in [\"질문_1\", \"질문_2\"]:\n",
    "        for a_col in [\"답변_1\", \"답변_2\", \"답변_3\", \"답변_4\", \"답변_5\"]:\n",
    "            questions.append(row[q_col])\n",
    "            answers.append(row[a_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0859ba14-2821-4d69-9ccf-90f104b84638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6gbcei1m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd74887b4ea48d0a95966fc748cdd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/870 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "creating metadata file for /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "loading configuration file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz2stwodv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d804e6702034fa29da61e9a15e9da59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "creating metadata file for /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "loading weights file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b828e48d204fcf802581a4dd4192c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"monologg/kobigbird-bert-base\"\n",
    "\n",
    "answer_embedder = AutoModel.from_pretrained(model_name) \n",
    "device = torch.device('cuda') \n",
    "\n",
    "a_input_ids, a_attn_masks = [], [] \n",
    "for i in tqdm(range(len(questions)), position=0, leave=True):\n",
    "    encoded_answer = tokenizer(answers[i], max_length=512, truncation=True, padding=\"max_length\") \n",
    "    a_input_ids.append(encoded_answer[\"input_ids\"]) \n",
    "    a_attn_masks.append(encoded_answer[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a62832-12bc-4a7c-aaa7-37a511bdca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_input_ids = torch.tensor(a_input_ids, dtype=int) \n",
    "a_attn_masks = torch.tensor(a_attn_masks, dtype=int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d3cf7a-b903-468a-a195-2e9aad1a5c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1885ffda2eae4a90aa6f000305c925cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_chkpt = torch.load(\"answer_embedder_DPR.pt\") \n",
    "\n",
    "answer_embedder.load_state_dict(answer_chkpt) \n",
    "answer_embedder.to(device)\n",
    "answer_embedder.eval() \n",
    "\n",
    "answer_embeddings = [] \n",
    "\n",
    "batch_size = 16\n",
    "answer_data = TensorDataset(a_input_ids, a_attn_masks) \n",
    "answer_sampler = SequentialSampler(answer_data) \n",
    "answer_dataloader = DataLoader(answer_data, sampler=answer_sampler, batch_size=batch_size) \n",
    "\n",
    "for batch in tqdm(answer_dataloader): \n",
    "    with torch.no_grad(): \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        answer_input_ids, answer_attn_masks = batch \n",
    "        answer_embedding = answer_embedder(answer_input_ids, answer_attn_masks).pooler_output \n",
    "        answer_embedding = answer_embedding.detach().cpu()\n",
    "        answer_embeddings.append(answer_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7adbbff9-435b-47aa-b8eb-5edffebc5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_embeddings = torch.cat(answer_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb0312b-0518-437f-b0e0-9140f4848460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6440, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235e9d11-f36d-47cc-be8a-ba7ddb1f9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_embeddings = answer_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a45122e4-8a4d-4524-b6c1-318f5e0b5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6440, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd762501-155b-42d4-b4ed-719c9b565e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test_question_embeddings = [] \n",
    "question_embedder = AutoModel.from_pretrained(model_name) \n",
    "question_chkpt = torch.load(\"question_embedder_DPR.pt\")\n",
    "question_embedder.to(device) \n",
    "question_embedder.load_state_dict(question_chkpt)\n",
    "question_embedder.eval() \n",
    "\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e917ab5-4cf3-42aa-8424-8ef4b3f25709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61ced6b6f3f40ce9ad3068606f4a2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_questions = test[\"질문\"].values \n",
    "\n",
    "q_input_ids, q_attn_masks = [], [] \n",
    "for i in tqdm(range(len(test_questions)), position=0, leave=True):\n",
    "    encoded_question = tokenizer(test_questions[i], max_length=512, truncation=True, padding=\"max_length\") \n",
    "    q_input_ids.append(encoded_question[\"input_ids\"]) \n",
    "    q_attn_masks.append(encoded_question[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73647c8-e868-4b6f-bfcc-8285fb0a2f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([130, 512]), torch.Size([130, 512]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_input_ids = torch.tensor(q_input_ids, dtype=int) \n",
    "q_attn_masks = torch.tensor(q_attn_masks, dtype=int) \n",
    "\n",
    "q_input_ids.shape, q_attn_masks.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d843bb64-0445-44e6-9e7e-e836f9dd115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934c61bbc20545f8acb023cf02bcff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_answers = [] \n",
    "\n",
    "for i in tqdm(range(len(q_input_ids))): \n",
    "    cur_input_ids = q_input_ids[i] \n",
    "    cur_attn_mask = q_attn_masks[i] \n",
    "    cur_input_ids = torch.reshape(cur_input_ids, (-1, 512)) \n",
    "    cur_attn_mask = torch.reshape(cur_attn_mask, (-1, 512)) \n",
    "    cur_input_ids = cur_input_ids.to(device) \n",
    "    cur_attn_mask = cur_attn_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = question_embedder(cur_input_ids, cur_attn_mask).pooler_output \n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(answer_embeddings, 0, 1))\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()  \n",
    "        top_answers.append(answers[rank[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f350563-e2b2-41f4-81c3-410529c95e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4667400-050a-4ba1-956f-4c1ad5f8c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f30b1773fc7475e85b4cc882be3f41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\".gitattributes\";:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6d47a79a0e444483a4f8c88b355695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ooling/config.json\";:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd346df1e9a49389e42e2737522076e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Dense/config.json\";:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be617ff764346348530a4ffae665dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959444ea4cf9483691e29aa618967a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"README.md\";:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0f5872a3444a628068373fdc9236d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"config.json\";:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff41ef263a1c40f8952aa07d34d3564e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_transformers.json\";:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2509b3bf15b44dda258c5d2b85da2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8763d1a03243e38ae7c3cc967b9974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e_bert_config.json\";:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b0604d9d7941ddba00bacfc059612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)al_tokens_map.json\";:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79636368ebbb4b90a8448f2b0ec5e4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059e465bd57d4834b3b342a8fbf5c533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enizer_config.json\";:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef954f428e0942059ebf3de0f9ce1f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"vocab.txt\";:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8a35b1bd4842bea3f6df6f44fe6f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"modules.json\";:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "Didn't find file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/added_tokens.json. We won't load it.\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/vocab.txt\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/tokenizer.json\n",
      "loading file None\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/special_tokens_map.json\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/tokenizer_config.json\n",
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130, 512)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = model.encode(top_answers)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7da3de87-0dd9-4f45-9631-cc65999bcb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.026506</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>-0.020368</td>\n",
       "      <td>0.093865</td>\n",
       "      <td>-0.055855</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.036171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029012</td>\n",
       "      <td>-0.028996</td>\n",
       "      <td>0.036807</td>\n",
       "      <td>-0.019275</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>0.093617</td>\n",
       "      <td>-0.016542</td>\n",
       "      <td>-0.036665</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>0.060282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>-0.034920</td>\n",
       "      <td>-0.051659</td>\n",
       "      <td>0.034640</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>-0.005048</td>\n",
       "      <td>-0.016672</td>\n",
       "      <td>0.024080</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016735</td>\n",
       "      <td>-0.024383</td>\n",
       "      <td>0.031203</td>\n",
       "      <td>-0.047914</td>\n",
       "      <td>-0.048928</td>\n",
       "      <td>0.049996</td>\n",
       "      <td>0.012761</td>\n",
       "      <td>-0.035183</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>-0.008451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>-0.106070</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.090208</td>\n",
       "      <td>-0.080892</td>\n",
       "      <td>-0.007853</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040023</td>\n",
       "      <td>-0.054669</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>-0.049280</td>\n",
       "      <td>0.015645</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>-0.031572</td>\n",
       "      <td>-0.012180</td>\n",
       "      <td>-0.015811</td>\n",
       "      <td>0.101507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>0.028780</td>\n",
       "      <td>-0.041766</td>\n",
       "      <td>0.038018</td>\n",
       "      <td>0.047464</td>\n",
       "      <td>-0.053030</td>\n",
       "      <td>-0.024172</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.024162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059843</td>\n",
       "      <td>-0.041408</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>-0.015097</td>\n",
       "      <td>-0.021498</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.026622</td>\n",
       "      <td>0.071312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>-0.059788</td>\n",
       "      <td>-0.020483</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>-0.035316</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>-0.014444</td>\n",
       "      <td>0.090898</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026803</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.061460</td>\n",
       "      <td>-0.013816</td>\n",
       "      <td>-0.008346</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.044158</td>\n",
       "      <td>0.045571</td>\n",
       "      <td>0.010090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "0  TEST_000  0.003907  0.026506  0.002935 -0.020368  0.093865 -0.055855   \n",
       "1  TEST_001 -0.034920 -0.051659  0.034640  0.006939  0.059443 -0.005048   \n",
       "2  TEST_002  0.014635  0.011680 -0.106070  0.010486  0.090208 -0.080892   \n",
       "3  TEST_003 -0.006627  0.028780 -0.041766  0.038018  0.047464 -0.053030   \n",
       "4  TEST_004 -0.059788 -0.020483  0.016567 -0.035316  0.002403 -0.014444   \n",
       "\n",
       "      vec_6     vec_7     vec_8  ...   vec_502   vec_503   vec_504   vec_505  \\\n",
       "0  0.022940  0.021612 -0.036171  ... -0.029012 -0.028996  0.036807 -0.019275   \n",
       "1 -0.016672  0.024080  0.012391  ... -0.016735 -0.024383  0.031203 -0.047914   \n",
       "2 -0.007853 -0.002600  0.010746  ... -0.040023 -0.054669  0.017772 -0.049280   \n",
       "3 -0.024172 -0.001850 -0.024162  ... -0.059843 -0.041408  0.010751 -0.015097   \n",
       "4  0.090898  0.006764  0.009712  ... -0.026803  0.013162 -0.000635  0.061460   \n",
       "\n",
       "    vec_506   vec_507   vec_508   vec_509   vec_510   vec_511  \n",
       "0 -0.000355  0.093617 -0.016542 -0.036665 -0.031433  0.060282  \n",
       "1 -0.048928  0.049996  0.012761 -0.035183  0.058803 -0.008451  \n",
       "2  0.015645  0.013285 -0.031572 -0.012180 -0.015811  0.101507  \n",
       "3 -0.021498  0.066948 -0.013049 -0.000514 -0.026622  0.071312  \n",
       "4 -0.013816 -0.008346  0.008502  0.044158  0.045571  0.010090  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[:,1:] = pred_embeddings\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "362b46a4-f159-4ce6-ba5c-ff3ea8079f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리더보드 제출을 위한 csv파일 생성\n",
    "submission.to_csv('raw_DPR.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b13bc-88e2-4977-b752-42b255118e13",
   "metadata": {},
   "source": [
    "## Concatenation of multiple answers (top 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2744093c-79a1-4082-b54f-851e3f98514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df75503517a945bba2b1731b4ad726b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_answers = [] \n",
    "\n",
    "for i in tqdm(range(len(q_input_ids))): \n",
    "    cur_input_ids = q_input_ids[i] \n",
    "    cur_attn_mask = q_attn_masks[i] \n",
    "    cur_input_ids = torch.reshape(cur_input_ids, (-1, 512)) \n",
    "    cur_attn_mask = torch.reshape(cur_attn_mask, (-1, 512)) \n",
    "    cur_input_ids = cur_input_ids.to(device) \n",
    "    cur_attn_mask = cur_attn_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = question_embedder(cur_input_ids, cur_attn_mask).pooler_output \n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(answer_embeddings, 0, 1))\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()  \n",
    "        top50_answers = [answers[rank[j]] for j in range(50)] \n",
    "        top50_answers = np.unique(top50_answers)\n",
    "        top_answers.append(top50_answers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "809e1af5-cea0-431c-8a84-754958f36593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffff30a225a04e9ebb65b158de2a1080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm25_filtered = [] \n",
    "\n",
    "for i in tqdm(range(len(test_questions))):\n",
    "    tokenized_corpus = [tokenizer.tokenize(doc) for doc in top_answers[i]]\n",
    "    tokenized_query = tokenizer.tokenize(test_questions[i])\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_doc_indices = np.argsort(doc_scores)[::-1][:5]\n",
    "    top_docs = [top_answers[i][index] for index in top_doc_indices]\n",
    "    concatenated_answer = \" \".join(top_docs)\n",
    "    bm25_filtered.append(concatenated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06c406d8-3329-44c9-a920-67e1a2b3bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "Didn't find file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/added_tokens.json. We won't load it.\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/vocab.txt\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/tokenizer.json\n",
      "loading file None\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/special_tokens_map.json\n",
      "loading file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/tokenizer_config.json\n",
      "loading configuration file /root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/sentence-transformers_distiluse-base-multilingual-cased-v1/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130, 512)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = model.encode(bm25_filtered)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eff4e9be-bbbc-411b-a4c2-b502e67a2923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>-0.009091</td>\n",
       "      <td>0.051201</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.006502</td>\n",
       "      <td>0.082316</td>\n",
       "      <td>-0.047824</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>-0.003625</td>\n",
       "      <td>-0.052473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050290</td>\n",
       "      <td>-0.047070</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>-0.060718</td>\n",
       "      <td>0.026424</td>\n",
       "      <td>0.072346</td>\n",
       "      <td>0.027770</td>\n",
       "      <td>-0.052403</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.080497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>-0.014106</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.013342</td>\n",
       "      <td>0.069398</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.023080</td>\n",
       "      <td>-0.008041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025134</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>-0.045904</td>\n",
       "      <td>-0.025309</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>-0.005466</td>\n",
       "      <td>-0.009464</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>-0.004162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>0.032605</td>\n",
       "      <td>-0.081564</td>\n",
       "      <td>-0.060735</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>0.096750</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>-0.012308</td>\n",
       "      <td>0.016761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037246</td>\n",
       "      <td>-0.021570</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.014931</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.056091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>-0.019172</td>\n",
       "      <td>0.027558</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>-0.047887</td>\n",
       "      <td>-0.058942</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031114</td>\n",
       "      <td>0.018074</td>\n",
       "      <td>0.069288</td>\n",
       "      <td>-0.063136</td>\n",
       "      <td>0.018898</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>-0.018820</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>0.058256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>-0.003082</td>\n",
       "      <td>-0.015159</td>\n",
       "      <td>-0.020049</td>\n",
       "      <td>0.104739</td>\n",
       "      <td>-0.020299</td>\n",
       "      <td>0.054620</td>\n",
       "      <td>0.043846</td>\n",
       "      <td>-0.014219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>0.025491</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>-0.036952</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.036506</td>\n",
       "      <td>-0.006303</td>\n",
       "      <td>0.050108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "0  TEST_000 -0.009091  0.051201  0.053451 -0.006502  0.082316 -0.047824   \n",
       "1  TEST_001  0.001539 -0.014106  0.006531  0.013342  0.069398 -0.013717   \n",
       "2  TEST_002  0.032605 -0.081564 -0.060735  0.009520  0.096750  0.006504   \n",
       "3  TEST_003  0.028997 -0.004509 -0.019172  0.027558  0.082488 -0.047887   \n",
       "4  TEST_004  0.014048 -0.003082 -0.015159 -0.020049  0.104739 -0.020299   \n",
       "\n",
       "      vec_6     vec_7     vec_8  ...   vec_502   vec_503   vec_504   vec_505  \\\n",
       "0  0.015298 -0.003625 -0.052473  ... -0.050290 -0.047070  0.027378 -0.060718   \n",
       "1 -0.008829 -0.023080 -0.008041  ... -0.025134 -0.016303  0.043635 -0.045904   \n",
       "2  0.027207 -0.012308  0.016761  ... -0.037246 -0.021570  0.024303 -0.029671   \n",
       "3 -0.058942  0.023962  0.018857  ... -0.031114  0.018074  0.069288 -0.063136   \n",
       "4  0.054620  0.043846 -0.014219  ... -0.003134 -0.026959  0.025491  0.006445   \n",
       "\n",
       "    vec_506   vec_507   vec_508   vec_509   vec_510   vec_511  \n",
       "0  0.026424  0.072346  0.027770 -0.052403 -0.003980  0.080497  \n",
       "1 -0.025309  0.035036 -0.005466 -0.009464  0.036712 -0.004162  \n",
       "2  0.014703  0.041096 -0.014370 -0.014931  0.010610  0.056091  \n",
       "3  0.018898  0.004010  0.007449 -0.018820 -0.026087  0.058256  \n",
       "4 -0.036952  0.005618  0.003694  0.036506 -0.006303  0.050108  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\") \n",
    "submission.iloc[:,1:] = pred_embeddings\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1467abb6-a8af-4161-931d-ad5e1b26d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>-0.009091</td>\n",
       "      <td>0.051201</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.006502</td>\n",
       "      <td>0.082316</td>\n",
       "      <td>-0.047824</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>-0.003625</td>\n",
       "      <td>-0.052473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050290</td>\n",
       "      <td>-0.047070</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>-0.060718</td>\n",
       "      <td>0.026424</td>\n",
       "      <td>0.072346</td>\n",
       "      <td>0.027770</td>\n",
       "      <td>-0.052403</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.080497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>-0.014106</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.013342</td>\n",
       "      <td>0.069398</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.023080</td>\n",
       "      <td>-0.008041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025134</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>-0.045904</td>\n",
       "      <td>-0.025309</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>-0.005466</td>\n",
       "      <td>-0.009464</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>-0.004162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>0.032605</td>\n",
       "      <td>-0.081564</td>\n",
       "      <td>-0.060735</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>0.096750</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.027207</td>\n",
       "      <td>-0.012308</td>\n",
       "      <td>0.016761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037246</td>\n",
       "      <td>-0.021570</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.014931</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.056091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>-0.019172</td>\n",
       "      <td>0.027558</td>\n",
       "      <td>0.082488</td>\n",
       "      <td>-0.047887</td>\n",
       "      <td>-0.058942</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031114</td>\n",
       "      <td>0.018074</td>\n",
       "      <td>0.069288</td>\n",
       "      <td>-0.063136</td>\n",
       "      <td>0.018898</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>-0.018820</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>0.058256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>-0.003082</td>\n",
       "      <td>-0.015159</td>\n",
       "      <td>-0.020049</td>\n",
       "      <td>0.104739</td>\n",
       "      <td>-0.020299</td>\n",
       "      <td>0.054620</td>\n",
       "      <td>0.043846</td>\n",
       "      <td>-0.014219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>-0.026959</td>\n",
       "      <td>0.025491</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>-0.036952</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.036506</td>\n",
       "      <td>-0.006303</td>\n",
       "      <td>0.050108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "0  TEST_000 -0.009091  0.051201  0.053451 -0.006502  0.082316 -0.047824   \n",
       "1  TEST_001  0.001539 -0.014106  0.006531  0.013342  0.069398 -0.013717   \n",
       "2  TEST_002  0.032605 -0.081564 -0.060735  0.009520  0.096750  0.006504   \n",
       "3  TEST_003  0.028997 -0.004509 -0.019172  0.027558  0.082488 -0.047887   \n",
       "4  TEST_004  0.014048 -0.003082 -0.015159 -0.020049  0.104739 -0.020299   \n",
       "\n",
       "      vec_6     vec_7     vec_8  ...   vec_502   vec_503   vec_504   vec_505  \\\n",
       "0  0.015298 -0.003625 -0.052473  ... -0.050290 -0.047070  0.027378 -0.060718   \n",
       "1 -0.008829 -0.023080 -0.008041  ... -0.025134 -0.016303  0.043635 -0.045904   \n",
       "2  0.027207 -0.012308  0.016761  ... -0.037246 -0.021570  0.024303 -0.029671   \n",
       "3 -0.058942  0.023962  0.018857  ... -0.031114  0.018074  0.069288 -0.063136   \n",
       "4  0.054620  0.043846 -0.014219  ... -0.003134 -0.026959  0.025491  0.006445   \n",
       "\n",
       "    vec_506   vec_507   vec_508   vec_509   vec_510   vec_511  \n",
       "0  0.026424  0.072346  0.027770 -0.052403 -0.003980  0.080497  \n",
       "1 -0.025309  0.035036 -0.005466 -0.009464  0.036712 -0.004162  \n",
       "2  0.014703  0.041096 -0.014370 -0.014931  0.010610  0.056091  \n",
       "3  0.018898  0.004010  0.007449 -0.018820 -0.026087  0.058256  \n",
       "4 -0.036952  0.005618  0.003694  0.036506 -0.006303  0.050108  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\") \n",
    "submission.iloc[:,1:] = pred_embeddings\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0098940-96d3-46dc-977c-643998bc6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"DPR_BM25_top5_concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d57e1-fa6a-4652-9338-37e0d5fbde19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
